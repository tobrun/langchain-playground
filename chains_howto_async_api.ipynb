{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async API\n",
    "\n",
    "LangChain provides async support for Chains by leveraging the asyncio library.\n",
    "\n",
    "Async methods are currently supported in LLMChain (through arun, apredict, acall) and LLMMathChain (through arun and acall), ChatVectorDBChain, and QA chains. Async support for other chains is on the roadmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CleanSmile Toothpaste.\n",
      "\n",
      "\n",
      "BrightSmile Toothpaste\n",
      "\n",
      "\n",
      "BrightBrush Toothpaste Co.\n",
      "\n",
      "\n",
      "BrightSmile Toothpaste Co.\n",
      "\n",
      "\n",
      "MintySmiles\n",
      "\u001b[1mConcurrent executed in 1.03 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "SparkleSmile Toothpaste Co.\n",
      "\n",
      "\n",
      "BrightSmile Toothpaste Co.\n",
      "\n",
      "\n",
      "Smile Solutions Toothpaste Co.\n",
      "\n",
      "\n",
      "BrightSmile Toothpaste Co.\n",
      "\n",
      "\n",
      "SmileBright Industries.\n",
      "\u001b[1mSerial executed in 3.03 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"product\"],\n",
    "        template=\"What is a good name for a company that makes {product}?\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    for _ in range(5):\n",
    "        resp = chain.run(product=\"toothpaste\")\n",
    "        print(resp)\n",
    "\n",
    "\n",
    "async def async_generate(chain):\n",
    "    resp = await chain.arun(product=\"toothpaste\")\n",
    "    print(resp)\n",
    "\n",
    "\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"product\"],\n",
    "        template=\"What is a good name for a company that makes {product}?\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    tasks = [async_generate(chain) for _ in range(5)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "s = time.perf_counter()\n",
    "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
    "await generate_concurrently()\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
    "\n",
    "s = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
